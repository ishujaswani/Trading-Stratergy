{
  "hash": "098436831c69ee17a759141b391de3a4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Optimisation\"\n---\n\n---\ntitle: \"Cleaning the data\"\n---\n\n**Importing**\n\n::: {#5171c299 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import gaussian_kde\nimport scipy\nimport scipy.sparse\nimport patsy\nfrom statistics import median\nimport bz2\nimport math\n```\n:::\n\n\n::: {#ca0675fc .cell execution_count=2}\n``` {.python .cell-code}\ndef sort_cols(test):\n    return(test.reindex(sorted(test.columns), axis=1))\n\n# Assuming model is in the same directory as the python file\n\n# update data from 2003 to 2010\n\nframes = {}\nfor year in [2003,2004, 2005, 2006,2007,2008,2009,2010]:\n    fil = \"pickle_data/FACTOR_MODEL/\" + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n    frames.update(pd.read_pickle(fil))\n\n\nfor x in frames: \n    frames[x] = sort_cols(frames[x])\n\n# Assuming model is in the same directory as the python file\n# update data from 2003 to 2010\ncovariance = {}\nfor year in [2003,2004, 2005, 2006,2007,2008,2009,2010]:\n    fil =  \"pickle_data/FACTOR_MODEL/\" + \"covariance.\" + str(year) + \".pickle.bz2\"\n    covariance.update(pd.read_pickle(fil))\n\nframes[\"20040102\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1DREVRSL</th>\n      <th>AERODEF</th>\n      <th>AIRLINES</th>\n      <th>ALUMSTEL</th>\n      <th>APPAREL</th>\n      <th>AUTO</th>\n      <th>BANKS</th>\n      <th>BETA</th>\n      <th>BEVTOB</th>\n      <th>BIOLIFE</th>\n      <th>...</th>\n      <th>SPTYSTOR</th>\n      <th>STREVRSL</th>\n      <th>SpecRisk</th>\n      <th>TELECOM</th>\n      <th>TRADECO</th>\n      <th>TRANSPRT</th>\n      <th>TotalRisk</th>\n      <th>VALUE</th>\n      <th>WIRELESS</th>\n      <th>Yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.032</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-2.177</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.548</td>\n      <td>9.014505</td>\n      <td>0.054</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.959397</td>\n      <td>0.644</td>\n      <td>0.0</td>\n      <td>0.188679</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.684</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-2.045</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.986</td>\n      <td>19.304651</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>23.393503</td>\n      <td>-0.700</td>\n      <td>0.0</td>\n      <td>3.203463</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.235</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-2.010</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.256</td>\n      <td>19.802556</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>22.135724</td>\n      <td>0.513</td>\n      <td>0.0</td>\n      <td>1.290796</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-0.948</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.841</td>\n      <td>31.274403</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.785120</td>\n      <td>0.609</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.674</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-2.148</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.588</td>\n      <td>13.533480</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>18.928129</td>\n      <td>2.986</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12430</th>\n      <td>-0.953</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.97</td>\n      <td>-2.009</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.430</td>\n      <td>9.095567</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.125982</td>\n      <td>0.928</td>\n      <td>0.0</td>\n      <td>2.770352</td>\n    </tr>\n    <tr>\n      <th>12431</th>\n      <td>-0.808</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.97</td>\n      <td>-2.066</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.561</td>\n      <td>8.739695</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.888183</td>\n      <td>0.966</td>\n      <td>0.0</td>\n      <td>7.175297</td>\n    </tr>\n    <tr>\n      <th>12432</th>\n      <td>-0.754</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.97</td>\n      <td>-2.053</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.734</td>\n      <td>9.565838</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.436052</td>\n      <td>0.818</td>\n      <td>0.0</td>\n      <td>2.770352</td>\n    </tr>\n    <tr>\n      <th>12433</th>\n      <td>-0.647</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.97</td>\n      <td>-2.022</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.680</td>\n      <td>9.394557</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>13.256003</td>\n      <td>0.774</td>\n      <td>0.0</td>\n      <td>2.770352</td>\n    </tr>\n    <tr>\n      <th>12434</th>\n      <td>0.331</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>-1.985</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>-0.519</td>\n      <td>9.591641</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.277461</td>\n      <td>0.316</td>\n      <td>0.0</td>\n      <td>4.148421</td>\n    </tr>\n  </tbody>\n</table>\n<p>12435 rows Ã— 93 columns</p>\n</div>\n```\n:::\n:::\n\n\n**Data Cleaning and Winsorization**\n\nThe distribution of many statistics can be heavily influenced by outliers. A simple approach to robustifying parameter estimation procedures is to set all outliers to a specified percentile of the data; for example, a 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. Winsorized estimators are usually more robust to outliers than their more standard forms.\n\n::: {#c0eb9d7c .cell execution_count=3}\n``` {.python .cell-code}\ndef wins(x, a, b):\n    return(np.where(x <= a, a, np.where(x >= b, b, x)))\n\ndef clean_nas(df):\n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for numeric_column in numeric_columns:\n        df[numeric_column] = np.nan_to_num(df[numeric_column])\n\n    return df\n```\n:::\n\n\n**Density Polt**\n\n::: {#20c2bb9a .cell execution_count=4}\n``` {.python .cell-code}\ndef density_plot(data, title):\n    density = gaussian_kde(data)\n    xs = np.linspace(np.min(data), np.max(data), 200)\n    density.covariance_factor = lambda: .25\n    density._compute_covariance()\n    plt.plot(xs, density(xs))\n    plt.title(title)\n    plt.show()\n\ntest = frames['20040102']\ndensity_plot(test['Ret'], 'Daily return pre-winsorization')\ndensity_plot(wins(test['Ret'], -0.25, 0.25), 'Daily return winsorized')\ndensity_plot(test['SpecRisk'] / (100 * math.sqrt(252)), 'SpecRisk')\n\n```\n\n::: {.cell-output .cell-output-display}\n![](about_files/figure-html/cell-5-output-1.png){width=571 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](about_files/figure-html/cell-5-output-2.png){width=579 height=431}\n:::\n\n::: {.cell-output .cell-output-display}\n![](about_files/figure-html/cell-5-output-3.png){width=566 height=431}\n:::\n:::\n\n\n**Data Definitions**\n\n- ID: a unique identifier that can be used to link stocks across time \\\n- 1DREVRSL: very short-term reversal, potential alpha factor but probably too fast- moving to be tradable \\\n- STREVRSL: short-term reversal, potential alpha factor \\\n- LTREVRSL: long-term reversal, potential alpha factor \\\n- BETA: risk factor computed from CAPM beta regression \\\n- EARNQLTY: earnings quality, potential alpha factor \\\n- EARNYILD: earnings yield (blend of forecasted earnings and historical earnings divided by market cap) \\\n- GROWTH: mix of historical and forecasted earnings growth \\\n- LEVERAGE: financial leverage of the company's balance sheet, usually a risk factor \\\n- LIQUIDTY: factor with high loadings for very liquidly traded names; usually a risk factor \\\n- MGMTQLTY: management quality, potential alpha factor which looks at quantitative measures of how well-run a company is by its management \\\n- MOMENTUM: 12-month growth in stock price, usually a risk factor \\\n- PROFIT: profitability, potential alpha factor \\\n- PROSPECT: based on skewness of the return distribution, potential risk factor RESVOL: risk factor computed from residual volatility \\\n- SEASON: seasonality-based alpha factor \\\n- SENTMT: news sentiment alpha factor \\\n- SIZE: risk factor based on log(market capitalization) \\\n- VALUE: risk factor based on ratio of tangible book value to current price \\\n- SpecRisk: specific risk is another name for predicted residual volatility. We called this the D matrix in our discussion of APT models. \\\n- TotalRisk: predicted total vol, including factor and idiosyncratic contributions, annualized \\\n- Ret: asset's total return on the next day after the factor loadings are known, suitable as the Y vector in a regression analysis \\\n- Yield: the dividend yield of the asset \\\n- HistBeta: historically estimated CAPM beta coefficient PredBeta: model-predicted beta coefficient in the future \\\n- INDMOM: industry momentum (defined as relative historical outperformance or underperformance of the other stocks in the same industry) \\\n- IssuerMarketCap: aggregate market capitalization of the company (all share classes from the same issuer, e.g. for Google would include both Alphabet A and C) \\\n- BidAskSpread: bid-offer spread (average for the day) \\\n- CompositeVolume: composite trading volume for the day \\\n- DataDate: the date when the data would have been known, as of the close\n\n\n> Many of the remaining columns are industry factors, of which a full list is given below.\n\n::: {#33f88909 .cell execution_count=5}\n``` {.python .cell-code}\nindustry_factors = ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO', 'BANKS', 'BEVTOB', \n'BIOLIFE', 'BLDGPROD', 'CHEM', 'CNSTENG', 'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB', 'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', \n'FOODRET', 'GASUTIL', 'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR', 'INDMACH', 'INSURNCE',\n'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH', 'MULTUTIL', 'OILGSCON', 'OILGSDRL', \n'OILGSEQP', 'OILGSEXP', 'PAPER', 'PHARMA', 'PRECMTLS', 'PSNLPROD', 'REALEST', 'RESTAUR', \n'ROADRAIL', 'SEMICOND', 'SEMIEQP', 'SOFTWARE', 'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', \n'TRADECO', 'TRANSPRT', 'WIRELESS']\n\nstyle_factors = ['BETA', 'SIZE', 'MOMENTUM', 'VALUE']\n\ndef get_formula(alpha):\n    L = [\"0\", alpha]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    return \"Ret ~ \" + \" + \".join(L)\n```\n:::\n\n\nTo solve Problem 1, we need to create a function named estimate_factor_returns that performs several key tasks:\n\nFilter the Data: The function should first filter the input dataframe df to include only rows where IssuerMarketCap > 1e9. This step ensures that the analysis focuses on liquid stocks.\n\nWinsorize the Return: The function should apply the winsorization process to the 'Ret' column of the dataframe. This step will mitigate the impact of outliers in the data.\n\nRun OLS Regression: Using the formula generated by get_formula function with the provided alpha factor, the function should perform an Ordinary Least Squares (OLS) regression. The dependent variable in this regression will be the winsorized return.\n\nReturn Results: The function should return the slope coefficients from the regression, and it can also return additional information from the regression results if desired.\n\nstructural exposures  - Security analysts \ncoeff - optimisation\n\n\n# problem 1\n\nWrite a function called \"estimate_factor_returns\" which takes as arguments, an alpha\nfactor name, and an input data frame \"df\". The function must not modify the contents\nof the data frame. The function will first subset to IssuerMarketCap > 1e9 to obtain a\nliquid universe, and then run an OLS regression using the formula returned by the get_formula function defined above. The dependent variable in the regression must be\nwinsorized return, using the winsorisation procedure discussed above in this\nnotebook. The estimate_factor_returns function should return an object which, at very\nleast, contains the slope coefficients from the regression, and may contain other\ninformation.\n\n::: {#009c8a91 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef estimate_factor_returns(alpha, df):\n    # Filtering data for IssuerMarketCap > 1e9\n    filtered_df = df[df['IssuerMarketCap'] > 1e9]\n\n    # Winsorizing the 'Ret' column\n    winsorized_ret = wins(filtered_df['Ret'], -0.25, 0.25) # ret is return\n\n    # Running OLS regression\n    formula = get_formula(alpha)\n    model = smf.ols(formula, data=filtered_df.assign(Ret=winsorized_ret)).fit()\n\n    # Returning the slope coefficients and optionally other information\n    return model.params\n\n# Example usage\nalpha_factor_name = 'PROFIT' \ndf = frames[\"20030103\"] \nresults = estimate_factor_returns(alpha_factor_name, df)\nprint(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPROFIT     -0.000699\nBETA       -0.000370\nSIZE       -0.001563\nMOMENTUM   -0.001272\nVALUE      -0.000369\n              ...   \nSPTYSTOR   -0.004445\nTELECOM    -0.011490\nTRADECO    -0.000577\nTRANSPRT   -0.006902\nWIRELESS    0.003033\nLength: 63, dtype: float64\n```\n:::\n:::\n\n\n- wins is the previously defined function for winsorization.\n- get_formula is the function provided in your notebook that generates the formula for the regression.\n- The dataframe df is expected to include all the necessary columns like 'IssuerMarketCap', 'Ret', and any columns relevant to the alpha factor and style/industry factors.\n\n# problem 2\nWrite a function which iteratively calls estimate_factor_returns on every data frame in\nthe dictionary called \"frames\" above, and returns a univariate time series containing\nthe coefficient on the alpha factor for each date. Since only the alpha factor return is\nsaved, style and industry factor returns are discarded by this procedure.\n\n::: {#b77dcb9d .cell execution_count=7}\n``` {.python .cell-code}\ndef iteratively_call_estimate_factor_return(frames, alpha_factors):\n    # DataFrame to store alpha factors and their corresponding time series\n    all_alpha_factor_returns = pd.DataFrame(index=frames.keys())\n\n    # Iterate through each date and DataFrame in the frames dictionary\n    for date, df in frames.items():\n        for alpha in alpha_factors:\n            # Call the estimate_factor_returns function for each alpha factor\n            factor_returns = estimate_factor_returns(alpha, df)\n            # Extract the alpha factor return and store it with the corresponding date\n            all_alpha_factor_returns.loc[date, alpha] = factor_returns.get(alpha, np.nan)\n\n    return all_alpha_factor_returns\n\n# Example usage\nalpha_factors = ['STREVRSL', 'PROFIT', 'SENTMT']\nalpha_time_series_df = iteratively_call_estimate_factor_return(frames, alpha_factors)\n```\n:::\n\n\n# problem 3\nFor each of the potential alpha factors STREVRSL, PROFIT, SENTMT, run the function\ncreated in Problem 2 and plot the cumulative sum of the resulting time series. Do any\nof these qualify as potential alpha factors? Do the cumulative returns from the plot\nrepresent returns on any tradable assets or portfolios?\n\n::: {#4b12108e .cell execution_count=8}\n``` {.python .cell-code}\nplt.plot(pd.Series(alpha_time_series_df[\"STREVRSL\"]).cumsum(), label=\"STREVRSL\")\nplt.plot(pd.Series(alpha_time_series_df[\"PROFIT\"]).cumsum(), label=\"PROFIT\")\nplt.plot(pd.Series(alpha_time_series_df[\"SENTMT\"]).cumsum(), label=\"SENTMT\")\n\nplt.title(\"Cumulative Returns of Potential Alpha Factors\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Returns\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.xticks(np.arange(0, len(alpha_time_series_df.index), 50), alpha_time_series_df.index[np.arange(0, len(alpha_time_series_df.index), 50)])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](about_files/figure-html/cell-9-output-1.png){width=591 height=494}\n:::\n:::\n\n\nAn alpha factor is a variable or set of variables that a model uses to predict stock returns. To qualify as a potential alpha factor, a variable should ideally show a consistent ability to generate excess returns over a benchmark.\n\nSTREVRSL shows a steep upward trend, which suggests that this factor might have been predictive of positive returns over the period shown. If these returns are risk-adjusted and beat a benchmark, STREVRSL could be considered a strong alpha factor.\n\nPROFIT and SENTMT exhibit a more modest upward trajectory. This suggests that while they may have some predictive power, their ability to generate excess returns isn't as pronounced as STREVRSL within the time frame of the data.\n\nPart 4: Do the cumulative returns from the plot represent returns on any tradable assets or portfolios?\nThe cumulative returns plotted for these alpha factors represent theoretical returns based on a model's predictions. They are not directly the returns of tradable assets but indicate how an investment strategy based on these factors might have performed. To translate these into actual tradable strategies:\n\nThe factors would need to be a part of a trading model that determines how much to invest in different assets.\n\nThe model would adjust its positions regularly based on updated factor scores and predictions.\n\nTransaction costs, slippage, market impact, and other real-world trading constraints would affect the actual returns realized.\n\nThe model's predictions would need to be converted into portfolio weights, and these weights would then be used to construct a portfolio of stocks or other assets.\n\nThe plotted cumulative returns suggest how a strategy solely based on these alpha factors might have accumulated gains over time, but translating these theoretical returns into actual tradable strategies requires a comprehensive investment model and consideration of trading costs and constraints. If the factors are robust and continue to perform well out-of-sample (i.e., in future unseen data), they might be incorporated into real trading strategies or investment portfolios.\n\n\n\n\n# Factors\n\n## Factor Exposures and Factor Returns\n\nArbitrage pricing theory relaxes several of the assumptions made in the course of deriving the CAPM. In particular, we relax the assumption that all investors do the same optimization and hence that there is a single efficient fund. This allows the possibility that a CAPM-like relation may hold, but with multiple underlying sources of risk.\n\nSpecifically, let $r_i, i = 1, \\ldots, n$ denote the cross-section of asset returns over a given time period $[t, t+1]$. In a fully-general model, the multivariate distribution $p(r)$ could have arbitrary covariance and higher-moment structures, but remember that for $n$ large there is typically never enough data to estimate such over-parameterized models.\n\nInstead, we assume a structural model which is the most direct generalization of the CAPM:\n\n$$ r_i = \\beta_{i,1} f_1 + \\beta_{i,2} f_2 + \\cdots + \\beta_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) $$\n\nIf $p = 1$, this reduces to the Capital Asset Pricing Model (CAPM) in a rather direct way.\n\nWith $p > 1$, the model starts to differ from the CAPM in several very important aspects. In the CAPM, we were able to identify the single efficient fund by arguing that its weights must equal the market-capitalization weights. Hence we were given for free a very nice proxy for the single efficient fund: a capitalization-weighted basket such as the Russell 3000. Hence in the $p = 1$ case we had a convenient proxy which could be used to impute the return $f_1$, which we called $r_M$. Also $\\beta_{i,1}$ could be estimated, with no more than the usual statistical estimation error, by time-series regression.\n\nIf $p > 1$ then the underlying assumptions of that argument break down: there is no longer any simple way to identify $f_j$ nor $\\beta_{i,j}$ (for $j = 1, \\ldots, p$). We shall return to the estimation problem in due course.\n\nTo avoid confusion with the CAPM, and its simplistic $\\beta$ coefficient (which is still sometimes used in larger multi-factor models), it is conventional to make the following notation change: $\\beta_{i,j}$ becomes $X_{i,j}$ and so the model equation becomes\n\n$$ r_i = X_{i,1} f_1 + X_{i,2} f_2 + \\cdots + X_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) $$\n\nItâ€™s difficult to simultaneously estimate both all components $X_{i,j}$ and all risk-source returns $f_j$, so one usually assumes one is known and calculates the other via regression. In what follows, we focus on the approach where $X$ is known, and the $f_j$ are assumed to be hidden (aka latent) variables.\n\nThe structural equation is more conveniently expressed in matrix form:\n\n$$ R_{t+1} = X_t f_{t+1} + \\epsilon_{t+1}, \\quad E[\\epsilon] = 0, \\quad V[\\epsilon] = D $$\n\nwhere $R_{t+1}$ is an $n$-dimensional random vector containing the cross-section of returns in excess of the risk-free rate over some time interval $[t, t + 1]$, and $X_t$ is a (non-random) $n \\times p$ matrix that can be calculated entirely from data known before time $t$. The variable $f$ denotes a $p$-dimensional random vector process which cannot be observed directly.\n\nSince the variable $f$ denotes a $p$-dimensional random vector process which cannot be observed directly, information about the $f$-process must be obtained via statistical inference. We assume that the $f$-process has finite first and second moments given by\n\n$$ E[f] = \\mu_f, \\quad V[f] = F $$.\n\nThe primary outputs of a statistical inference process are the parameters $\\mu_f$ and $F$, and other outputs one might be interested in include estimates of the daily realizations $\\hat{f}_{t+1}$.\n\nThe simplest way of estimating historical daily realizations of $\\hat{f}_{t+1}$ is by least-squares (ordinary or weighted, as appropriate), viewing the defining model equation as a regression problem.\n\n::: {#56c84a93 .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport patsy\n\ndef get_estu(df):\n    \"\"\"Estimation universe definition\"\"\"\n    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n    return estu\n\ndef colnames(X):\n    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n    if type(X) == patsy.design_info.DesignMatrix:\n        return X.design_info.column_names\n    if type(X) == pd.core.frame.DataFrame:\n        return X.columns.tolist()\n    return None\n\ndef diagonal_factor_cov(date, X):\n    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n    cv = covariance[date]\n    k = np.shape(X)[1]\n    Fm = np.zeros([k,k])\n    for j in range(0,k):\n        fac = colnames(X)[j]\n        Fm[j,j] = (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"].iloc[0]\n    return Fm\n\ndef risk_exposures(estu):\n    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n    L = [\"0\"]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    my_formula = \" + \".join(L)\n    return patsy.dmatrix(my_formula, data = estu)\n```\n:::\n\n\n### Helpful code to show how to get X, F, D matrices\n\n::: {#f5fc4daf .cell execution_count=10}\n``` {.python .cell-code}\nmy_date = '20040102'\n# estu = estimation universe\nestu = get_estu(frames[my_date])\nestu['Ret'] = wins(estu['Ret'], -0.25, 0.25)\nrske = risk_exposures(estu)\nF = diagonal_factor_cov(my_date, rske)\nX = np.asarray(rske)\nD = np.asarray( (estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2 )\nkappa = 1e-5\ncandidate_alphas = [\n    'STREVRSL', 'LTREVRSL', 'INDMOM',\n    'EARNQLTY', 'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']\n```\n:::\n\n\n### problem 0\nAll of the below pertain to the estimation universe as defined above. Modify the daily data frames, removing all non-estimation-universe rows, before continuing.\n\n### Problem 1. Residual Returns\n\nWithin each daily data frame, let $Y$ denote the residuals of the variable `Ret`,\nwith respect to the risk model. In other words, define\n\n$$ Y := \\text{Ret} - XX^+\\text{Ret} $$\n\nwhere $X^+$ denotes the pseudoinverse, and $X$ is constructed as above (i.e., using the `risk_exposures` function). Augment the data frames you have been given, by adding a new column, $Y$, to each frame. Be sure to winsorize the `Ret` column prior to computing $Y$ as above. You do not have to save the augmented data, unless you want to. In other words, the modification that adds column $Y$ can be done in-memory.\n\n### Problem 2. Model Selection\n\nSplit your data into a training/validation set $D_{\\text{train}}$, and an ultimate test set (vault), $D_{\\text{test}}$. Do not split within a single day; rather, some dates end up in $D_{\\text{train}}$ and the rest in $D_{\\text{test}}$. This will be the basis of your cross-validation study later on.\n\nIt will be helpful to join together vertically the frames in the training/validation set $D_{\\text{train}}$ into a single frame called a panel. For the avoidance of doubt, the panel will have the same columns as any one of the daily frames individually, and the panel will have a large number of rows (the sum of all the rows of all the frames in $D_{\\text{train}}$).\n\nConsider the list of candidate alpha factors given above. Find a model of the form $Y = f(\\text{candidate alphas}) + \\epsilon$\nwhere $Y$ is the residual return from above. Determine the function $f()$ using cross-validation to optimize any tunable hyper-parameters. First, to get started, assume $f$ is linear and use lasso or elastic net cross-validation tools (e.g., from sklearn). Then, get creative and try at least one non-linear functional form for $f$, again using cross-validation to optimize any tunable hyper-parameters.\n\n### Problem 3. Efficient Portfolio Optimization\n\nCode up the efficient formula for portfolio optimization discussed in lecture, based on the Woodbury matrix inversion lemma.\n\n### Problem 4. Putting it All Together\n\nUsing the helpful code example above, and using the output of the function $f$ as your final alpha factor, construct a backtest of a portfolio optimization strategy. In other words, compute the optimal portfolio each day, and dot product it with `Ret` to get the pre-tcost 1-day profit for each day. Use the previous problem to speed things up. Create time-series plots of the long market value, short market value, and cumulative profit of this portfolio sequence. Also plot the daily risk, in dollars, of your portfolios and the percent of the risk that is idiosyncratic.\n\n",
    "supporting": [
      "about_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}