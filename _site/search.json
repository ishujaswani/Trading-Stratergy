[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Trading Analysis Project",
    "section": "",
    "text": "This project focuses on advanced trading analysis, emphasizing meticulous data cleaning, robust optimization methods, and strategic portfolio management. Our aim is to use statistical and machine learning techniques to refine trading strategies, enhance risk management, and guide investment decisions effectively."
  },
  {
    "objectID": "about.html#factor-exposures-and-factor-returns",
    "href": "about.html#factor-exposures-and-factor-returns",
    "title": "Cleaning the data",
    "section": "Factor Exposures and Factor Returns",
    "text": "Factor Exposures and Factor Returns\nArbitrage pricing theory relaxes several of the assumptions made in the course of deriving the CAPM. In particular, we relax the assumption that all investors do the same optimization and hence that there is a single efficient fund. This allows the possibility that a CAPM-like relation may hold, but with multiple underlying sources of risk.\nSpecifically, let \\(r_i, i = 1, \\ldots, n\\) denote the cross-section of asset returns over a given time period \\([t, t+1]\\). In a fully-general model, the multivariate distribution \\(p(r)\\) could have arbitrary covariance and higher-moment structures, but remember that for \\(n\\) large there is typically never enough data to estimate such over-parameterized models.\nInstead, we assume a structural model which is the most direct generalization of the CAPM:\n\\[ r_i = \\beta_{i,1} f_1 + \\beta_{i,2} f_2 + \\cdots + \\beta_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIf \\(p = 1\\), this reduces to the Capital Asset Pricing Model (CAPM) in a rather direct way.\nWith \\(p &gt; 1\\), the model starts to differ from the CAPM in several very important aspects. In the CAPM, we were able to identify the single efficient fund by arguing that its weights must equal the market-capitalization weights. Hence we were given for free a very nice proxy for the single efficient fund: a capitalization-weighted basket such as the Russell 3000. Hence in the \\(p = 1\\) case we had a convenient proxy which could be used to impute the return \\(f_1\\), which we called \\(r_M\\). Also \\(\\beta_{i,1}\\) could be estimated, with no more than the usual statistical estimation error, by time-series regression.\nIf \\(p &gt; 1\\) then the underlying assumptions of that argument break down: there is no longer any simple way to identify \\(f_j\\) nor \\(\\beta_{i,j}\\) (for \\(j = 1, \\ldots, p\\)). We shall return to the estimation problem in due course.\nTo avoid confusion with the CAPM, and its simplistic \\(\\beta\\) coefficient (which is still sometimes used in larger multi-factor models), it is conventional to make the following notation change: \\(\\beta_{i,j}\\) becomes \\(X_{i,j}\\) and so the model equation becomes\n\\[ r_i = X_{i,1} f_1 + X_{i,2} f_2 + \\cdots + X_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIt’s difficult to simultaneously estimate both all components \\(X_{i,j}\\) and all risk-source returns \\(f_j\\), so one usually assumes one is known and calculates the other via regression. In what follows, we focus on the approach where \\(X\\) is known, and the \\(f_j\\) are assumed to be hidden (aka latent) variables.\nThe structural equation is more conveniently expressed in matrix form:\n\\[ R_{t+1} = X_t f_{t+1} + \\epsilon_{t+1}, \\quad E[\\epsilon] = 0, \\quad V[\\epsilon] = D \\]\nwhere \\(R_{t+1}\\) is an \\(n\\)-dimensional random vector containing the cross-section of returns in excess of the risk-free rate over some time interval \\([t, t + 1]\\), and \\(X_t\\) is a (non-random) \\(n \\times p\\) matrix that can be calculated entirely from data known before time \\(t\\). The variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly.\nSince the variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly, information about the \\(f\\)-process must be obtained via statistical inference. We assume that the \\(f\\)-process has finite first and second moments given by\n\\[ E[f] = \\mu_f, \\quad V[f] = F \\].\nThe primary outputs of a statistical inference process are the parameters \\(\\mu_f\\) and \\(F\\), and other outputs one might be interested in include estimates of the daily realizations \\(\\hat{f}_{t+1}\\).\nThe simplest way of estimating historical daily realizations of \\(\\hat{f}_{t+1}\\) is by least-squares (ordinary or weighted, as appropriate), viewing the defining model equation as a regression problem.\n\nimport pandas as pd\nimport patsy\n\ndef get_estu(df):\n    \"\"\"Estimation universe definition\"\"\"\n    estu = df.loc[df.IssuerMarketCap &gt; 1e9].copy(deep=True)\n    return estu\n\ndef colnames(X):\n    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n    if type(X) == patsy.design_info.DesignMatrix:\n        return X.design_info.column_names\n    if type(X) == pd.core.frame.DataFrame:\n        return X.columns.tolist()\n    return None\n\ndef diagonal_factor_cov(date, X):\n    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n    cv = covariance[date]\n    k = np.shape(X)[1]\n    Fm = np.zeros([k,k])\n    for j in range(0,k):\n        fac = colnames(X)[j]\n        Fm[j,j] = (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"].iloc[0]\n    return Fm\n\ndef risk_exposures(estu):\n    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n    L = [\"0\"]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    my_formula = \" + \".join(L)\n    return patsy.dmatrix(my_formula, data = estu)\n\n\nHelpful code to show how to get X, F, D matrices\n\nmy_date = '20040102'\n# estu = estimation universe\nestu = get_estu(frames[my_date])\nestu['Ret'] = wins(estu['Ret'], -0.25, 0.25)\nrske = risk_exposures(estu)\nF = diagonal_factor_cov(my_date, rske)\nX = np.asarray(rske)\nD = np.asarray( (estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2 )\nkappa = 1e-5\ncandidate_alphas = [\n    'STREVRSL', 'LTREVRSL', 'INDMOM',\n    'EARNQLTY', 'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']\n\n\n\nproblem 0\nAll of the below pertain to the estimation universe as defined above. Modify the daily data frames, removing all non-estimation-universe rows, before continuing.\n\n\nProblem 1. Residual Returns\nWithin each daily data frame, let \\(Y\\) denote the residuals of the variable Ret, with respect to the risk model. In other words, define\n\\[ Y := \\text{Ret} - XX^+\\text{Ret} \\]\nwhere \\(X^+\\) denotes the pseudoinverse, and \\(X\\) is constructed as above (i.e., using the risk_exposures function). Augment the data frames you have been given, by adding a new column, \\(Y\\), to each frame. Be sure to winsorize the Ret column prior to computing \\(Y\\) as above. You do not have to save the augmented data, unless you want to. In other words, the modification that adds column \\(Y\\) can be done in-memory.\n\n\nProblem 2. Model Selection\nSplit your data into a training/validation set \\(D_{\\text{train}}\\), and an ultimate test set (vault), \\(D_{\\text{test}}\\). Do not split within a single day; rather, some dates end up in \\(D_{\\text{train}}\\) and the rest in \\(D_{\\text{test}}\\). This will be the basis of your cross-validation study later on.\nIt will be helpful to join together vertically the frames in the training/validation set \\(D_{\\text{train}}\\) into a single frame called a panel. For the avoidance of doubt, the panel will have the same columns as any one of the daily frames individually, and the panel will have a large number of rows (the sum of all the rows of all the frames in \\(D_{\\text{train}}\\)).\nConsider the list of candidate alpha factors given above. Find a model of the form \\(Y = f(\\text{candidate alphas}) + \\epsilon\\) where \\(Y\\) is the residual return from above. Determine the function \\(f()\\) using cross-validation to optimize any tunable hyper-parameters. First, to get started, assume \\(f\\) is linear and use lasso or elastic net cross-validation tools (e.g., from sklearn). Then, get creative and try at least one non-linear functional form for \\(f\\), again using cross-validation to optimize any tunable hyper-parameters.\n\n\nProblem 3. Efficient Portfolio Optimization\nCode up the efficient formula for portfolio optimization discussed in lecture, based on the Woodbury matrix inversion lemma.\n\n\nProblem 4. Putting it All Together\nUsing the helpful code example above, and using the output of the function \\(f\\) as your final alpha factor, construct a backtest of a portfolio optimization strategy. In other words, compute the optimal portfolio each day, and dot product it with Ret to get the pre-tcost 1-day profit for each day. Use the previous problem to speed things up. Create time-series plots of the long market value, short market value, and cumulative profit of this portfolio sequence. Also plot the daily risk, in dollars, of your portfolios and the percent of the risk that is idiosyncratic."
  },
  {
    "objectID": "about.html#project-overview",
    "href": "about.html#project-overview",
    "title": "About the Trading Analysis Project",
    "section": "",
    "text": "This project focuses on advanced trading analysis, emphasizing meticulous data cleaning, robust optimization methods, and strategic portfolio management. Our aim is to use statistical and machine learning techniques to refine trading strategies, enhance risk management, and guide investment decisions effectively."
  },
  {
    "objectID": "about.html#key-components",
    "href": "about.html#key-components",
    "title": "About the Trading Analysis Project",
    "section": "Key Components",
    "text": "Key Components\n\nData Cleaning\nIn financial analysis, the integrity and quality of data are crucial. Our approach to data cleaning involves several critical steps:\n\nHandling Missing Values: We address missing data with techniques like imputation and interpolation to ensure our datasets are complete, enhancing the reliability of our analyses.\nOutlier Treatment: Outliers can significantly distort statistical analyses. We employ winsorization to limit their impact, setting extreme values to a specified percentile, thus maintaining a distribution closer to the majority of the data.\nData Standardization: Ensuring consistency in our data metrics is essential for reliable analysis. We standardize data across various scales to enable accurate comparisons and aggregations.\n\n\n\nOptimisation Techniques\nThe project employs several optimization methods for both model selection and portfolio construction:\n\nModel Selection: Using Lasso and Elastic Net regression, we identify significant alpha factors from a range of potential predictors, ensuring that our models focus on the most impactful variables. We also explore non-linear models, such as XGBoost, to capture complex patterns and relationships in the data.\nEfficient Portfolio Optimization: We use the Woodbury Matrix Inversion Lemma within the framework of Markowitz’s mean-variance optimization to construct portfolios. This approach balances the trade-off between maximizing returns and minimizing risk.\n\n\n\nPortfolio Management\nOur dynamic portfolio management system includes:\n\nAnalyzing Residual Returns: We focus on the residuals of asset returns to identify unique alpha opportunities, seeking profits beyond what the market typically offers.\nImplementing Backtesting: Through rigorous backtesting, we assess the performance of our models and strategies over historical data, providing insights into their effectiveness and robustness over time.\nRisk Management: We actively calculate and monitor risk metrics, including total and idiosyncratic risk, to manage our portfolios effectively, ensuring that our strategies are aligned with our risk tolerance levels.\n\n\n\nTools and Technologies\n\nPython: Our primary tool for data manipulation, statistical modeling, and machine learning.\nQuarto: Used for documenting our analysis in a clear, reproducible format.\nPandas & NumPy: Essential for efficient data processing and numerical operations.\nStatsmodels & Scikit-Learn: For conducting regression analyses and model validation.\nXGBoost: An advanced machine learning tool for capturing more complex data patterns.\nMatplotlib & Seaborn: For visualizing data and insights."
  },
  {
    "objectID": "about.html#project-outcomes",
    "href": "about.html#project-outcomes",
    "title": "About the Trading Analysis Project",
    "section": "Project Outcomes",
    "text": "Project Outcomes\nThis project aims to deliver:\n\nRobust Trading Strategies: By leveraging data-driven insights and advanced analytical methods, we aim to develop more effective and resilient trading strategies.\nImproved Risk Management: Through our sophisticated risk assessment and portfolio optimization methods, we aim to enhance the management of investment risks.\nTransparent Analysis: We provide comprehensive documentation of our methodologies and findings, ensuring that our analysis is transparent and reproducible."
  },
  {
    "objectID": "about.html#future-directions",
    "href": "about.html#future-directions",
    "title": "About the Trading Analysis Project",
    "section": "Future Directions",
    "text": "Future Directions\nOur future plans include:\n\nIncorporating Real-Time Data: To adapt our strategies to the current market conditions and dynamics.\nExploring Alternative Models: Investigating deep learning and other advanced techniques for more nuanced and accurate market predictions.\nExpanding Asset Coverage: Diversifying our strategies across various asset classes and geographies to manage risk and explore new opportunities.\n\n\nOur project stands at the intersection of finance and technology, aiming to revolutionize trading strategies through data-driven insights and sophisticated statistical models. We strive for continuous improvement in trading performance and risk management, adapting to the ever-evolving financial landscape."
  },
  {
    "objectID": "optimisation.html",
    "href": "optimisation.html",
    "title": "Optimisation",
    "section": "",
    "text": "Arbitrage pricing theory relaxes several of the assumptions made in the course of deriving the CAPM. In particular, we relax the assumption that all investors do the same optimization and hence that there is a single efficient fund. This allows the possibility that a CAPM-like relation may hold, but with multiple underlying sources of risk.\nSpecifically, let \\(r_i, i = 1, \\ldots, n\\) denote the cross-section of asset returns over a given time period \\([t, t+1]\\). In a fully-general model, the multivariate distribution \\(p(r)\\) could have arbitrary covariance and higher-moment structures, but remember that for \\(n\\) large there is typically never enough data to estimate such over-parameterized models.\nInstead, we assume a structural model which is the most direct generalization of the CAPM:\n\\[ r_i = \\beta_{i,1} f_1 + \\beta_{i,2} f_2 + \\cdots + \\beta_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIf \\(p = 1\\), this reduces to the Capital Asset Pricing Model (CAPM) in a rather direct way.\nWith \\(p &gt; 1\\), the model starts to differ from the CAPM in several very important aspects. In the CAPM, we were able to identify the single efficient fund by arguing that its weights must equal the market-capitalization weights. Hence we were given for free a very nice proxy for the single efficient fund: a capitalization-weighted basket such as the Russell 3000. Hence in the \\(p = 1\\) case we had a convenient proxy which could be used to impute the return \\(f_1\\), which we called \\(r_M\\). Also \\(\\beta_{i,1}\\) could be estimated, with no more than the usual statistical estimation error, by time-series regression.\nIf \\(p &gt; 1\\) then the underlying assumptions of that argument break down: there is no longer any simple way to identify \\(f_j\\) nor \\(\\beta_{i,j}\\) (for \\(j = 1, \\ldots, p\\)). We shall return to the estimation problem in due course.\nTo avoid confusion with the CAPM, and its simplistic \\(\\beta\\) coefficient (which is still sometimes used in larger multi-factor models), it is conventional to make the following notation change: \\(\\beta_{i,j}\\) becomes \\(X_{i,j}\\) and so the model equation becomes\n\\[ r_i = X_{i,1} f_1 + X_{i,2} f_2 + \\cdots + X_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIt’s difficult to simultaneously estimate both all components \\(X_{i,j}\\) and all risk-source returns \\(f_j\\), so one usually assumes one is known and calculates the other via regression. In what follows, we focus on the approach where \\(X\\) is known, and the \\(f_j\\) are assumed to be hidden (aka latent) variables.\nThe structural equation is more conveniently expressed in matrix form:\n\\[ R_{t+1} = X_t f_{t+1} + \\epsilon_{t+1}, \\quad E[\\epsilon] = 0, \\quad V[\\epsilon] = D \\]\nwhere \\(R_{t+1}\\) is an \\(n\\)-dimensional random vector containing the cross-section of returns in excess of the risk-free rate over some time interval \\([t, t + 1]\\), and \\(X_t\\) is a (non-random) \\(n \\times p\\) matrix that can be calculated entirely from data known before time \\(t\\). The variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly.\nSince the variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly, information about the \\(f\\)-process must be obtained via statistical inference. We assume that the \\(f\\)-process has finite first and second moments given by\n\\[ E[f] = \\mu_f, \\quad V[f] = F \\]\nThe primary outputs of a statistical inference process are the parameters \\(\\mu_f\\) and \\(F\\), and other outputs one might be interested in include estimates of the daily realizations \\(\\hat{f}_{t+1}\\).\nThe simplest way of estimating historical daily realizations of \\(\\hat{f}_{t+1}\\) is by least-squares (ordinary or weighted, as appropriate), viewing the defining model equation as a regression problem.\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n\ndef sort_cols(test):\n    return(test.reindex(sorted(test.columns), axis=1))\n\n# Assuming model is in the same directory as the python file\n\n# update data from 2003 to 2010\n\nframes = {}\nfor year in [2003,2004,2005,2006,2007,2008,2009,2010]:\n    fil = \"pickle_data/FACTOR_MODEL/\" + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n    frames.update(pd.read_pickle(fil))\n\n\nfor x in frames: \n    frames[x] = sort_cols(frames[x])\n\n# Assuming model is in the same directory as the python file\n# update data from 2003 to 2010\ncovariance = {}\nfor year in [2003,2004,2005,2006,2007,2008,2009,2010]:\n    fil =  \"pickle_data/FACTOR_MODEL/\" + \"covariance.\" + str(year) + \".pickle.bz2\"\n    covariance.update(pd.read_pickle(fil))\n\n\nimport pandas as pd\nimport patsy\n\ndef wins(x, a, b):\n    return(np.where(x &lt;= a, a, np.where(x &gt;= b, b, x)))\n\ndef clean_nas(df):\n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for numeric_column in numeric_columns:\n        df[numeric_column] = np.nan_to_num(df[numeric_column])\n\n    return df\n\n\nindustry_factors = ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO', 'BANKS', 'BEVTOB', \n'BIOLIFE', 'BLDGPROD', 'CHEM', 'CNSTENG', 'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB', 'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', \n'FOODRET', 'GASUTIL', 'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR', 'INDMACH', 'INSURNCE',\n'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH', 'MULTUTIL', 'OILGSCON', 'OILGSDRL', \n'OILGSEQP', 'OILGSEXP', 'PAPER', 'PHARMA', 'PRECMTLS', 'PSNLPROD', 'REALEST', 'RESTAUR', \n'ROADRAIL', 'SEMICOND', 'SEMIEQP', 'SOFTWARE', 'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', \n'TRADECO', 'TRANSPRT', 'WIRELESS']\n\nstyle_factors = ['BETA', 'SIZE', 'MOMENTUM', 'VALUE']\n\ndef get_formula(alpha):\n    L = [\"0\", alpha]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    return \"Ret ~ \" + \" + \".join(L)\n\n\ndef get_estu(df):\n    \"\"\"Estimation universe definition\"\"\"\n    estu = df.loc[df.IssuerMarketCap &gt; 1e9].copy(deep=True)\n    return estu\n\ndef colnames(X):\n    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n    if type(X) == patsy.design_info.DesignMatrix:\n        return X.design_info.column_names\n    if type(X) == pd.core.frame.DataFrame:\n        return X.columns.tolist()\n    return None\n\ndef diagonal_factor_cov(date, X):\n    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n    cv = covariance[date]\n    k = np.shape(X)[1]\n    Fm = np.zeros([k,k])\n    for j in range(0,k):\n        fac = colnames(X)[j]\n        Fm[j,j] = (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"].iloc[0]\n    return (Fm,k)\n\ndef risk_exposures(estu):\n    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n    L = [\"0\"]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    my_formula = \" + \".join(L)\n    return patsy.dmatrix(my_formula, data = estu)\n\n\n\n\nmy_date = '20101231'\n# estu = estimation universe\nestu = get_estu(frames[my_date])\nestu['Ret'] = wins(estu['Ret'], -0.25, 0.25)\nrske = risk_exposures(estu)\nF = diagonal_factor_cov(my_date, rske)\nX = np.asarray(rske)\nD = np.asarray( (estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2 )\nkappa = 1e-5\ncandidate_alphas = [\n    'STREVRSL', 'LTREVRSL', 'INDMOM',\n    'EARNQLTY', 'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']\n\n\n\n\nAll of the below pertain to the estimation universe as defined above. Modify the daily data frames, removing all non-estimation-universe rows, before continuing.\n\n# updated frames dataset for the new estu universe\nfor date in frames:\n    frames[date] = get_estu(frames[date])\n\n\n\n\nWithin each daily data frame, let \\(Y\\) denote the residuals of the variable Ret, with respect to the risk model. In other words, define\n\\[ Y := \\text{Ret} - XX^+\\text{Ret} \\]\nwhere \\(X^+\\) denotes the pseudoinverse, and \\(X\\) is constructed as above (i.e., using the risk_exposures function). Augment the data frames you have been given, by adding a new column, \\(Y\\), to each frame. Be sure to winsorize the Ret column prior to computing \\(Y\\) as above. You do not have to save the augmented data, unless you want to. In other words, the modification that adds column \\(Y\\) can be done in-memory.\n\n# pseudoinverse function\ndef pinv(A):\n    \n    # Compute SVD of the matrix\n    U, s, Vt = np.linalg.svd(A,full_matrices=False)\n\n    # Step 2: Create the pseudoinverse of the diagonal matrix Sigma\n    s_pinv = np.zeros(s.shape) # creating an empty matrix to store invesed diagonals in\n    for i in range(len(s)):\n        if s[i] != 0: # Consider only diagonals\n            s_pinv[i] = 1.0 / s[i] # inverse the diagonals \n        \n    # Compute the pseudoinverse of A using the formula from equation 2 listed above\n    A_pinv = Vt.T @ np.diag(s_pinv) @ U.T\n        \n    return A_pinv\n\n\nfor date in frames:\n    # Winsorize the 'Ret' column\n    frames[date]['Ret'] = wins(frames[date]['Ret'], -0.25, 0.25)\n\n    # Compute the Risk Exposure Matrix (X)\n    estu = get_estu(frames[date])\n    rske = risk_exposures(estu)\n    X = np.asarray(rske)\n\n    # Compute the Pseudoinverse (X^+)\n    X_pinv = pinv(X)\n\n    # Calculate Residuals (Y)\n    Y = estu['Ret'] - np.matmul(X, np.matmul(X_pinv, estu['Ret']))\n\n    # Augment the Data Frames with the calculated Y\n    estu['Y'] = Y\n\n    # Update the frame in the dictionary\n    frames[date] = estu\n\n\n\n\nSplit your data into a training/validation set \\(D_{\\text{train}}\\), and an ultimate test set (vault), \\(D_{\\text{test}}\\). Do not split within a single day; rather, some dates end up in \\(D_{\\text{train}}\\) and the rest in \\(D_{\\text{test}}\\). This will be the basis of your cross-validation study later on.\nIt will be helpful to join together vertically the frames in the training/validation set \\(D_{\\text{train}}\\) into a single frame called a panel. For the avoidance of doubt, the panel will have the same columns as any one of the daily frames individually, and the panel will have a large number of rows (the sum of all the rows of all the frames in \\(D_{\\text{train}}\\)).\nConsider the list of candidate alpha factors given above. Find a model of the form \\(Y = f(\\text{candidate alphas}) + \\epsilon\\) where \\(Y\\) is the residual return from above. Determine the function \\(f()\\) using cross-validation to optimize any tunable hyper-parameters. First, to get started, assume \\(f\\) is linear and use lasso or elastic net cross-validation tools (e.g., from sklearn). Then, get creative and try at least one non-linear functional form for \\(f\\), again using cross-validation to optimize any tunable hyper-parameters.\n\n\n\n\n\nimport pandas as pd\n\ndef create_panel_data(frames, split_ratio=0.8):\n    \"\"\"\n    Splits the data into training and test sets and creates panel data for each.\n\n    Parameters:\n    - frames: Dictionary with dates as keys and data frames as values.\n    - split_ratio: Float representing the percentage of data to be used for training.\n\n    Returns:\n    - train_panel: Panel data for training set.\n    - test_panel: Panel data for test set.\n    \"\"\"\n\n    # Sort the dates to ensure chronological splitting\n    sorted_dates = sorted(frames.keys())\n\n    # Calculate the index to split the data\n    split_index = int(split_ratio * len(sorted_dates))\n\n    # Split the dates into training and testing\n    train_dates = sorted_dates[:split_index]\n    test_dates = sorted_dates[split_index:]\n\n    # Extract frames for training and testing\n    train_frames = {date: frames[date] for date in train_dates}\n    test_frames = {date: frames[date] for date in test_dates}\n\n    # Concatenate the frames vertically to create panels\n    train_panel = pd.concat(train_frames.values())\n    test_panel = pd.concat(test_frames.values())\n\n    return train_panel, test_panel\n\n# Usage example\ntrain_panel, test_panel = create_panel_data(frames, split_ratio=0.8)\n\nY = train_panel[\"Y\"]\nx = train_panel[candidate_alphas]\n\nX_test = test_panel[candidate_alphas]\nY_test = test_panel[\"Y\"]\n\n\nprint(\"Shape of X:\", x.shape)\nprint(\"Shape of Y:\", Y.shape)\n\nShape of X: (3955790, 9)\nShape of Y: (3955790,)\n\n\n\ndef get_candidate_alpha_coefficients_from_model(model_coef, candidate_alphas):\n    \"\"\"\n    Extracts coefficients of candidate alphas from a given model.\n\n    Parameters:\n    model_coef (pd.Series): Coefficients from the model.\n    candidate_alphas (list): List of candidate alpha factors.\n\n    Returns:\n    pd.Series: Coefficients for candidate alphas.\n    \"\"\"\n    return model_coef[candidate_alphas]"
  },
  {
    "objectID": "optimisation.html#factor-exposures-and-factor-returns",
    "href": "optimisation.html#factor-exposures-and-factor-returns",
    "title": "Optimisation",
    "section": "",
    "text": "Arbitrage pricing theory relaxes several of the assumptions made in the course of deriving the CAPM. In particular, we relax the assumption that all investors do the same optimization and hence that there is a single efficient fund. This allows the possibility that a CAPM-like relation may hold, but with multiple underlying sources of risk.\nSpecifically, let \\(r_i, i = 1, \\ldots, n\\) denote the cross-section of asset returns over a given time period \\([t, t+1]\\). In a fully-general model, the multivariate distribution \\(p(r)\\) could have arbitrary covariance and higher-moment structures, but remember that for \\(n\\) large there is typically never enough data to estimate such over-parameterized models.\nInstead, we assume a structural model which is the most direct generalization of the CAPM:\n\\[ r_i = \\beta_{i,1} f_1 + \\beta_{i,2} f_2 + \\cdots + \\beta_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIf \\(p = 1\\), this reduces to the Capital Asset Pricing Model (CAPM) in a rather direct way.\nWith \\(p &gt; 1\\), the model starts to differ from the CAPM in several very important aspects. In the CAPM, we were able to identify the single efficient fund by arguing that its weights must equal the market-capitalization weights. Hence we were given for free a very nice proxy for the single efficient fund: a capitalization-weighted basket such as the Russell 3000. Hence in the \\(p = 1\\) case we had a convenient proxy which could be used to impute the return \\(f_1\\), which we called \\(r_M\\). Also \\(\\beta_{i,1}\\) could be estimated, with no more than the usual statistical estimation error, by time-series regression.\nIf \\(p &gt; 1\\) then the underlying assumptions of that argument break down: there is no longer any simple way to identify \\(f_j\\) nor \\(\\beta_{i,j}\\) (for \\(j = 1, \\ldots, p\\)). We shall return to the estimation problem in due course.\nTo avoid confusion with the CAPM, and its simplistic \\(\\beta\\) coefficient (which is still sometimes used in larger multi-factor models), it is conventional to make the following notation change: \\(\\beta_{i,j}\\) becomes \\(X_{i,j}\\) and so the model equation becomes\n\\[ r_i = X_{i,1} f_1 + X_{i,2} f_2 + \\cdots + X_{i,p} f_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma_i^2) \\]\nIt’s difficult to simultaneously estimate both all components \\(X_{i,j}\\) and all risk-source returns \\(f_j\\), so one usually assumes one is known and calculates the other via regression. In what follows, we focus on the approach where \\(X\\) is known, and the \\(f_j\\) are assumed to be hidden (aka latent) variables.\nThe structural equation is more conveniently expressed in matrix form:\n\\[ R_{t+1} = X_t f_{t+1} + \\epsilon_{t+1}, \\quad E[\\epsilon] = 0, \\quad V[\\epsilon] = D \\]\nwhere \\(R_{t+1}\\) is an \\(n\\)-dimensional random vector containing the cross-section of returns in excess of the risk-free rate over some time interval \\([t, t + 1]\\), and \\(X_t\\) is a (non-random) \\(n \\times p\\) matrix that can be calculated entirely from data known before time \\(t\\). The variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly.\nSince the variable \\(f\\) denotes a \\(p\\)-dimensional random vector process which cannot be observed directly, information about the \\(f\\)-process must be obtained via statistical inference. We assume that the \\(f\\)-process has finite first and second moments given by\n\\[ E[f] = \\mu_f, \\quad V[f] = F \\]\nThe primary outputs of a statistical inference process are the parameters \\(\\mu_f\\) and \\(F\\), and other outputs one might be interested in include estimates of the daily realizations \\(\\hat{f}_{t+1}\\).\nThe simplest way of estimating historical daily realizations of \\(\\hat{f}_{t+1}\\) is by least-squares (ordinary or weighted, as appropriate), viewing the defining model equation as a regression problem.\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n\ndef sort_cols(test):\n    return(test.reindex(sorted(test.columns), axis=1))\n\n# Assuming model is in the same directory as the python file\n\n# update data from 2003 to 2010\n\nframes = {}\nfor year in [2003,2004,2005,2006,2007,2008,2009,2010]:\n    fil = \"pickle_data/FACTOR_MODEL/\" + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n    frames.update(pd.read_pickle(fil))\n\n\nfor x in frames: \n    frames[x] = sort_cols(frames[x])\n\n# Assuming model is in the same directory as the python file\n# update data from 2003 to 2010\ncovariance = {}\nfor year in [2003,2004,2005,2006,2007,2008,2009,2010]:\n    fil =  \"pickle_data/FACTOR_MODEL/\" + \"covariance.\" + str(year) + \".pickle.bz2\"\n    covariance.update(pd.read_pickle(fil))\n\n\nimport pandas as pd\nimport patsy\n\ndef wins(x, a, b):\n    return(np.where(x &lt;= a, a, np.where(x &gt;= b, b, x)))\n\ndef clean_nas(df):\n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for numeric_column in numeric_columns:\n        df[numeric_column] = np.nan_to_num(df[numeric_column])\n\n    return df\n\n\nindustry_factors = ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO', 'BANKS', 'BEVTOB', \n'BIOLIFE', 'BLDGPROD', 'CHEM', 'CNSTENG', 'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB', 'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', \n'FOODRET', 'GASUTIL', 'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR', 'INDMACH', 'INSURNCE',\n'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH', 'MULTUTIL', 'OILGSCON', 'OILGSDRL', \n'OILGSEQP', 'OILGSEXP', 'PAPER', 'PHARMA', 'PRECMTLS', 'PSNLPROD', 'REALEST', 'RESTAUR', \n'ROADRAIL', 'SEMICOND', 'SEMIEQP', 'SOFTWARE', 'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', \n'TRADECO', 'TRANSPRT', 'WIRELESS']\n\nstyle_factors = ['BETA', 'SIZE', 'MOMENTUM', 'VALUE']\n\ndef get_formula(alpha):\n    L = [\"0\", alpha]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    return \"Ret ~ \" + \" + \".join(L)\n\n\ndef get_estu(df):\n    \"\"\"Estimation universe definition\"\"\"\n    estu = df.loc[df.IssuerMarketCap &gt; 1e9].copy(deep=True)\n    return estu\n\ndef colnames(X):\n    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n    if type(X) == patsy.design_info.DesignMatrix:\n        return X.design_info.column_names\n    if type(X) == pd.core.frame.DataFrame:\n        return X.columns.tolist()\n    return None\n\ndef diagonal_factor_cov(date, X):\n    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n    cv = covariance[date]\n    k = np.shape(X)[1]\n    Fm = np.zeros([k,k])\n    for j in range(0,k):\n        fac = colnames(X)[j]\n        Fm[j,j] = (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"].iloc[0]\n    return (Fm,k)\n\ndef risk_exposures(estu):\n    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n    L = [\"0\"]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    my_formula = \" + \".join(L)\n    return patsy.dmatrix(my_formula, data = estu)\n\n\n\n\nmy_date = '20101231'\n# estu = estimation universe\nestu = get_estu(frames[my_date])\nestu['Ret'] = wins(estu['Ret'], -0.25, 0.25)\nrske = risk_exposures(estu)\nF = diagonal_factor_cov(my_date, rske)\nX = np.asarray(rske)\nD = np.asarray( (estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2 )\nkappa = 1e-5\ncandidate_alphas = [\n    'STREVRSL', 'LTREVRSL', 'INDMOM',\n    'EARNQLTY', 'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']\n\n\n\n\nAll of the below pertain to the estimation universe as defined above. Modify the daily data frames, removing all non-estimation-universe rows, before continuing.\n\n# updated frames dataset for the new estu universe\nfor date in frames:\n    frames[date] = get_estu(frames[date])\n\n\n\n\nWithin each daily data frame, let \\(Y\\) denote the residuals of the variable Ret, with respect to the risk model. In other words, define\n\\[ Y := \\text{Ret} - XX^+\\text{Ret} \\]\nwhere \\(X^+\\) denotes the pseudoinverse, and \\(X\\) is constructed as above (i.e., using the risk_exposures function). Augment the data frames you have been given, by adding a new column, \\(Y\\), to each frame. Be sure to winsorize the Ret column prior to computing \\(Y\\) as above. You do not have to save the augmented data, unless you want to. In other words, the modification that adds column \\(Y\\) can be done in-memory.\n\n# pseudoinverse function\ndef pinv(A):\n    \n    # Compute SVD of the matrix\n    U, s, Vt = np.linalg.svd(A,full_matrices=False)\n\n    # Step 2: Create the pseudoinverse of the diagonal matrix Sigma\n    s_pinv = np.zeros(s.shape) # creating an empty matrix to store invesed diagonals in\n    for i in range(len(s)):\n        if s[i] != 0: # Consider only diagonals\n            s_pinv[i] = 1.0 / s[i] # inverse the diagonals \n        \n    # Compute the pseudoinverse of A using the formula from equation 2 listed above\n    A_pinv = Vt.T @ np.diag(s_pinv) @ U.T\n        \n    return A_pinv\n\n\nfor date in frames:\n    # Winsorize the 'Ret' column\n    frames[date]['Ret'] = wins(frames[date]['Ret'], -0.25, 0.25)\n\n    # Compute the Risk Exposure Matrix (X)\n    estu = get_estu(frames[date])\n    rske = risk_exposures(estu)\n    X = np.asarray(rske)\n\n    # Compute the Pseudoinverse (X^+)\n    X_pinv = pinv(X)\n\n    # Calculate Residuals (Y)\n    Y = estu['Ret'] - np.matmul(X, np.matmul(X_pinv, estu['Ret']))\n\n    # Augment the Data Frames with the calculated Y\n    estu['Y'] = Y\n\n    # Update the frame in the dictionary\n    frames[date] = estu\n\n\n\n\nSplit your data into a training/validation set \\(D_{\\text{train}}\\), and an ultimate test set (vault), \\(D_{\\text{test}}\\). Do not split within a single day; rather, some dates end up in \\(D_{\\text{train}}\\) and the rest in \\(D_{\\text{test}}\\). This will be the basis of your cross-validation study later on.\nIt will be helpful to join together vertically the frames in the training/validation set \\(D_{\\text{train}}\\) into a single frame called a panel. For the avoidance of doubt, the panel will have the same columns as any one of the daily frames individually, and the panel will have a large number of rows (the sum of all the rows of all the frames in \\(D_{\\text{train}}\\)).\nConsider the list of candidate alpha factors given above. Find a model of the form \\(Y = f(\\text{candidate alphas}) + \\epsilon\\) where \\(Y\\) is the residual return from above. Determine the function \\(f()\\) using cross-validation to optimize any tunable hyper-parameters. First, to get started, assume \\(f\\) is linear and use lasso or elastic net cross-validation tools (e.g., from sklearn). Then, get creative and try at least one non-linear functional form for \\(f\\), again using cross-validation to optimize any tunable hyper-parameters."
  },
  {
    "objectID": "optimisation.html#part-1",
    "href": "optimisation.html#part-1",
    "title": "Optimisation",
    "section": "",
    "text": "import pandas as pd\n\ndef create_panel_data(frames, split_ratio=0.8):\n    \"\"\"\n    Splits the data into training and test sets and creates panel data for each.\n\n    Parameters:\n    - frames: Dictionary with dates as keys and data frames as values.\n    - split_ratio: Float representing the percentage of data to be used for training.\n\n    Returns:\n    - train_panel: Panel data for training set.\n    - test_panel: Panel data for test set.\n    \"\"\"\n\n    # Sort the dates to ensure chronological splitting\n    sorted_dates = sorted(frames.keys())\n\n    # Calculate the index to split the data\n    split_index = int(split_ratio * len(sorted_dates))\n\n    # Split the dates into training and testing\n    train_dates = sorted_dates[:split_index]\n    test_dates = sorted_dates[split_index:]\n\n    # Extract frames for training and testing\n    train_frames = {date: frames[date] for date in train_dates}\n    test_frames = {date: frames[date] for date in test_dates}\n\n    # Concatenate the frames vertically to create panels\n    train_panel = pd.concat(train_frames.values())\n    test_panel = pd.concat(test_frames.values())\n\n    return train_panel, test_panel\n\n# Usage example\ntrain_panel, test_panel = create_panel_data(frames, split_ratio=0.8)\n\nY = train_panel[\"Y\"]\nx = train_panel[candidate_alphas]\n\nX_test = test_panel[candidate_alphas]\nY_test = test_panel[\"Y\"]\n\n\nprint(\"Shape of X:\", x.shape)\nprint(\"Shape of Y:\", Y.shape)\n\nShape of X: (3955790, 9)\nShape of Y: (3955790,)\n\n\n\ndef get_candidate_alpha_coefficients_from_model(model_coef, candidate_alphas):\n    \"\"\"\n    Extracts coefficients of candidate alphas from a given model.\n\n    Parameters:\n    model_coef (pd.Series): Coefficients from the model.\n    candidate_alphas (list): List of candidate alpha factors.\n\n    Returns:\n    pd.Series: Coefficients for candidate alphas.\n    \"\"\"\n    return model_coef[candidate_alphas]"
  },
  {
    "objectID": "optimisation.html#markowitz-model-mathematical-foundation",
    "href": "optimisation.html#markowitz-model-mathematical-foundation",
    "title": "Optimisation",
    "section": "Markowitz Model: Mathematical Foundation",
    "text": "Markowitz Model: Mathematical Foundation\n\nObjective\nThe goal is to construct a portfolio that offers the maximum expected return for a given level of risk or the minimum risk for a given level of expected return.\n\n\nVariables\n\n\\(h\\): Vector of portfolio weights.\n\\(r\\): Vector of expected asset returns.\n\\(\\Omega\\): Covariance matrix of asset returns.\n\n\n\nExpected Portfolio Return\n\n\\(E[h' r]\\), where \\(h'\\) denotes the transpose of \\(h\\).\n\n\n\nPortfolio Variance (Risk)\n\n\\(h' \\Omega h\\).\n\n\n\nOptimization Problem\nMaximize \\(E[h' r] - \\frac{1}{2\\kappa} h' \\Omega h\\), where \\(\\kappa\\) is a risk-aversion parameter. This form assumes a quadratic utility function which represents the trade-off between risk and return.\n\n\nSolution\nUnder the assumption that \\(\\Omega\\) is invertible, the optimal portfolio weights \\(h^*\\) can be found as \\(h^* = \\frac{1}{\\kappa} \\Omega^{-1} E[r]\\).\n\n\nMean-Variance Formulation\nThe problem simplifies to Maximize over \\(h\\): \\(h' E[r] - \\frac{1}{2\\kappa} h' \\Omega h\\)."
  },
  {
    "objectID": "optimisation.html#woodbury-matrix-inversion-lemma",
    "href": "optimisation.html#woodbury-matrix-inversion-lemma",
    "title": "Optimisation",
    "section": "Woodbury Matrix Inversion Lemma",
    "text": "Woodbury Matrix Inversion Lemma\nThis lemma provides an efficient way to compute the inverse of a matrix that has been modified by a rank-k update. The formula is:\n\\((A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + VA^{-1} U)^{-1} VA^{-1}\\)\nWhere: - \\(A\\) is a square matrix. - \\(U, C, V\\) are matrices of appropriate dimensions."
  },
  {
    "objectID": "optimisation.html#application-in-portfolio-optimization",
    "href": "optimisation.html#application-in-portfolio-optimization",
    "title": "Optimisation",
    "section": "Application in Portfolio Optimization",
    "text": "Application in Portfolio Optimization\nApplying the Woodbury lemma to the covariance matrix in the Markowitz model can significantly speed up computations, especially for large portfolios.\n\nIn the Portfolio Context\n\nLet \\(A = D\\), \\(U = X\\), \\(V = X'\\), and \\(C = F\\) in the Woodbury lemma.\n\\(D\\) represents a diagonal matrix (typically of variances), \\(X\\) is a factor loading matrix, and \\(F\\) is the covariance matrix of factors.\n\n\n\nEfficient Inverse Calculation\nBy using the Woodbury lemma, one can efficiently compute \\(\\Omega^{-1}\\), which is crucial for determining the optimal portfolio weights in the Markowitz model.\n\nimport scipy.optimize\n\n# Woodbury Matrix Inversion Lemma Function\ndef woodbury_inverse(A, U, C, V):\n    A_inv = np.linalg.inv(A)\n    M = np.linalg.inv(C + V @ A_inv @ U)\n    return A_inv - A_inv @ U @ M @ V @ A_inv\n\n# Portfolio Optimization Function\ndef optimize_portfolio(date, frames, covariance, kappa, alpha_factors):\n    estu = get_estu(frames[date])\n    n = estu.shape[0]\n    # print(f\"Number of assets in EstU: {n}\")\n\n    estu['Ret'] = wins(estu['Ret'], -0.25, 0.25)\n\n    # Combine alpha factors into a single alpha score\n    estu['alpha'] = sum([estu[factor] * coefficient for factor, coefficient in alpha_factors.items()])\n    estu_alpha = estu['alpha'].to_numpy()\n\n    # Risk exposures and factor covariance\n    rske = risk_exposures(estu)\n    # print(f\"Dimension of risk exposures (rske): {rske.shape}\")\n\n    Fm,k = diagonal_factor_cov(date, rske)\n\n    A = np.diag(np.diag(Fm))\n    U = Fm - A\n    V = U.T\n    C = np.eye(k)\n\n    F = woodbury_inverse(A,U,C,V)\n\n    X = np.asarray(rske)\n    D = np.asarray((estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2)\n\n    # Optimization objective function\n    def fun(h): \n        x = X.T @ h\n        obj = -np.dot(h, estu_alpha) + 0.5 * kappa * (np.dot(x, F @ x) + np.dot(h ** 2, D))\n        return obj\n\n    # Gradient of the objective function\n    def grad_f(h): \n        return -estu_alpha + kappa * (X @ (F @ (X.T @ h)) + D * h)\n\n    # Optimization using L-BFGS-B algorithm\n    optimizer_result = scipy.optimize.fmin_l_bfgs_b(fun, np.zeros(n), fprime=grad_f)\n    return optimizer_result\n\n\n\nProblem 4. Putting it All Together\nUsing the helpful code example above, and using the output of the function \\(f\\) as your final alpha factor, construct a backtest of a portfolio optimization strategy. In other words, compute the optimal portfolio each day, and dot product it with Ret to get the pre-tcost 1-day profit for each day. Use the previous problem to speed things up. Create time-series plots of the long market value, short market value, and cumulative profit of this portfolio sequence. Also plot the daily risk, in dollars, of your portfolios and the percent of the risk that is idiosyncratic.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# Your existing code with modifications for risk calculations and direct ls_ratio calculation\ncumulative_profit = []\nlong_market_value = []\nshort_market_value = []\ndaily_risk = []\nidiosyncratic_risk_percent = []\nls_ratio_list = []  # To store the long-short ratio\n\nfor date in sorted(frames.keys()):\n    optimal_portfolio = optimize_portfolio(date, frames, covariance, kappa, selected_alphas_lasso)\n    optimal_portfolio = optimal_portfolio[0] / sum(np.abs(optimal_portfolio[0]))\n    \n    # Calculate portfolio metrics\n    daily_return = optimal_portfolio @ frames[date]['Ret']\n    cumulative_profit.append(daily_return)\n    \n    long_mv = np.sum(optimal_portfolio[optimal_portfolio &gt; 0])\n    short_mv = np.sum(optimal_portfolio[optimal_portfolio &lt; 0])\n    long_market_value.append(long_mv)\n    short_market_value.append(short_mv)\n    \n    # Calculate long-short ratio directly in the loop\n    ls_ratio = np.abs(long_mv / short_mv) if short_mv != 0 else np.inf\n    ls_ratio_list.append(ls_ratio)\n\n    # Total risk and idiosyncratic risk calculations\n    total_risk = np.abs(optimal_portfolio) @ (frames[date]['TotalRisk'] / (100 * math.sqrt(252)))\n    idio_risk = 1 - (np.abs(optimal_portfolio) @ (frames[date]['SpecRisk'] / (100 * math.sqrt(252)))) / total_risk\n    daily_risk.append(total_risk)\n    idiosyncratic_risk_percent.append(idio_risk * 100)  # Convert to percentage\n\n# Convert lists to time series\ndate_index = pd.to_datetime(sorted(frames.keys()))\ncumulative_profit_ts = pd.Series(cumulative_profit, index=date_index).cumsum()\nlong_market_value_ts = pd.Series(long_market_value, index=date_index)\nshort_market_value_ts = pd.Series(short_market_value, index=date_index)\ndaily_risk_ts = pd.Series(daily_risk, index=date_index)\nidiosyncratic_risk_percent_ts = pd.Series(idiosyncratic_risk_percent, index=date_index)\nls_ratio_ts = pd.Series(ls_ratio_list, index=date_index)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cleaning the data",
    "section": "",
    "text": "Importing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import gaussian_kde\nimport scipy\nimport scipy.sparse\nimport patsy\nfrom statistics import median\nimport bz2\nimport math\n\n\ndef sort_cols(test):\n    return(test.reindex(sorted(test.columns), axis=1))\n\n# Assuming model is in the same directory as the python file\n\n# update data from 2003 to 2010\n\nframes = {}\nfor year in [2003,2004, 2005, 2006,2007,2008,2009,2010]:\n    fil = \"pickle_data/FACTOR_MODEL/\" + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n    frames.update(pd.read_pickle(fil))\n\n\nfor x in frames: \n    frames[x] = sort_cols(frames[x])\n\n# Assuming model is in the same directory as the python file\n# update data from 2003 to 2010\ncovariance = {}\nfor year in [2003,2004, 2005, 2006,2007,2008,2009,2010]:\n    fil =  \"pickle_data/FACTOR_MODEL/\" + \"covariance.\" + str(year) + \".pickle.bz2\"\n    covariance.update(pd.read_pickle(fil))\n\nframes[\"20040102\"]\n\n\n\n\n\n\n\n\n1DREVRSL\nAERODEF\nAIRLINES\nALUMSTEL\nAPPAREL\nAUTO\nBANKS\nBETA\nBEVTOB\nBIOLIFE\n...\nSPTYSTOR\nSTREVRSL\nSpecRisk\nTELECOM\nTRADECO\nTRANSPRT\nTotalRisk\nVALUE\nWIRELESS\nYield\n\n\n\n\n0\n-0.032\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-2.177\n0.0\n0.0\n...\n0.0\n0.548\n9.014505\n0.054\n0.0\n0.0\n13.959397\n0.644\n0.0\n0.188679\n\n\n1\n0.684\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-2.045\n0.0\n0.0\n...\n0.0\n0.986\n19.304651\n0.000\n0.0\n0.0\n23.393503\n-0.700\n0.0\n3.203463\n\n\n2\n0.235\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-2.010\n0.0\n0.0\n...\n0.0\n-0.256\n19.802556\n0.000\n0.0\n0.0\n22.135724\n0.513\n0.0\n1.290796\n\n\n3\n0.759\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-0.948\n0.0\n0.0\n...\n0.0\n1.841\n31.274403\n0.000\n0.0\n0.0\n35.785120\n0.609\n0.0\nNaN\n\n\n4\n0.674\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-2.148\n0.0\n0.0\n...\n0.0\n-0.588\n13.533480\n0.000\n0.0\n0.0\n18.928129\n2.986\n0.0\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12430\n-0.953\n0.0\n0.0\n0.0\n0.0\n0.0\n0.97\n-2.009\n0.0\n0.0\n...\n0.0\n0.430\n9.095567\n0.000\n0.0\n0.0\n13.125982\n0.928\n0.0\n2.770352\n\n\n12431\n-0.808\n0.0\n0.0\n0.0\n0.0\n0.0\n0.97\n-2.066\n0.0\n0.0\n...\n0.0\n0.561\n8.739695\n0.000\n0.0\n0.0\n12.888183\n0.966\n0.0\n7.175297\n\n\n12432\n-0.754\n0.0\n0.0\n0.0\n0.0\n0.0\n0.97\n-2.053\n0.0\n0.0\n...\n0.0\n0.734\n9.565838\n0.000\n0.0\n0.0\n13.436052\n0.818\n0.0\n2.770352\n\n\n12433\n-0.647\n0.0\n0.0\n0.0\n0.0\n0.0\n0.97\n-2.022\n0.0\n0.0\n...\n0.0\n0.680\n9.394557\n0.000\n0.0\n0.0\n13.256003\n0.774\n0.0\n2.770352\n\n\n12434\n0.331\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n-1.985\n0.0\n0.0\n...\n0.0\n-0.519\n9.591641\n0.000\n0.0\n0.0\n12.277461\n0.316\n0.0\n4.148421\n\n\n\n\n12435 rows × 93 columns\n\n\n\nData Cleaning and Winsorization\nThe distribution of many statistics can be heavily influenced by outliers. A simple approach to robustifying parameter estimation procedures is to set all outliers to a specified percentile of the data; for example, a 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile. Winsorized estimators are usually more robust to outliers than their more standard forms.\n\ndef wins(x, a, b):\n    return(np.where(x &lt;= a, a, np.where(x &gt;= b, b, x)))\n\ndef clean_nas(df):\n    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    for numeric_column in numeric_columns:\n        df[numeric_column] = np.nan_to_num(df[numeric_column])\n\n    return df\n\nDensity Polt\n\ndef density_plot(data, title):\n    density = gaussian_kde(data)\n    xs = np.linspace(np.min(data), np.max(data), 200)\n    density.covariance_factor = lambda: .25\n    density._compute_covariance()\n    plt.plot(xs, density(xs))\n    plt.title(title)\n    plt.show()\n\ntest = frames['20040102']\ndensity_plot(test['Ret'], 'Daily return pre-winsorization')\ndensity_plot(wins(test['Ret'], -0.25, 0.25), 'Daily return winsorized')\ndensity_plot(test['SpecRisk'] / (100 * math.sqrt(252)), 'SpecRisk')\n\n\n\n\n\n\n\n\n\n\nData Definitions\n\nID: a unique identifier that can be used to link stocks across time\n\n1DREVRSL: very short-term reversal, potential alpha factor but probably too fast- moving to be tradable\n\nSTREVRSL: short-term reversal, potential alpha factor\n\nLTREVRSL: long-term reversal, potential alpha factor\n\nBETA: risk factor computed from CAPM beta regression\n\nEARNQLTY: earnings quality, potential alpha factor\n\nEARNYILD: earnings yield (blend of forecasted earnings and historical earnings divided by market cap)\n\nGROWTH: mix of historical and forecasted earnings growth\n\nLEVERAGE: financial leverage of the company’s balance sheet, usually a risk factor\n\nLIQUIDTY: factor with high loadings for very liquidly traded names; usually a risk factor\n\nMGMTQLTY: management quality, potential alpha factor which looks at quantitative measures of how well-run a company is by its management\n\nMOMENTUM: 12-month growth in stock price, usually a risk factor\n\nPROFIT: profitability, potential alpha factor\n\nPROSPECT: based on skewness of the return distribution, potential risk factor RESVOL: risk factor computed from residual volatility\n\nSEASON: seasonality-based alpha factor\n\nSENTMT: news sentiment alpha factor\n\nSIZE: risk factor based on log(market capitalization)\n\nVALUE: risk factor based on ratio of tangible book value to current price\n\nSpecRisk: specific risk is another name for predicted residual volatility. We called this the D matrix in our discussion of APT models.\n\nTotalRisk: predicted total vol, including factor and idiosyncratic contributions, annualized\n\nRet: asset’s total return on the next day after the factor loadings are known, suitable as the Y vector in a regression analysis\n\nYield: the dividend yield of the asset\n\nHistBeta: historically estimated CAPM beta coefficient PredBeta: model-predicted beta coefficient in the future\n\nINDMOM: industry momentum (defined as relative historical outperformance or underperformance of the other stocks in the same industry)\n\nIssuerMarketCap: aggregate market capitalization of the company (all share classes from the same issuer, e.g. for Google would include both Alphabet A and C)\n\nBidAskSpread: bid-offer spread (average for the day)\n\nCompositeVolume: composite trading volume for the day\n\nDataDate: the date when the data would have been known, as of the close\n\n\nMany of the remaining columns are industry factors, of which a full list is given below.\n\n\nindustry_factors = ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO', 'BANKS', 'BEVTOB', \n'BIOLIFE', 'BLDGPROD', 'CHEM', 'CNSTENG', 'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB', 'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', \n'FOODRET', 'GASUTIL', 'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR', 'INDMACH', 'INSURNCE',\n'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH', 'MULTUTIL', 'OILGSCON', 'OILGSDRL', \n'OILGSEQP', 'OILGSEXP', 'PAPER', 'PHARMA', 'PRECMTLS', 'PSNLPROD', 'REALEST', 'RESTAUR', \n'ROADRAIL', 'SEMICOND', 'SEMIEQP', 'SOFTWARE', 'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', \n'TRADECO', 'TRANSPRT', 'WIRELESS']\n\nstyle_factors = ['BETA', 'SIZE', 'MOMENTUM', 'VALUE']\n\ndef get_formula(alpha):\n    L = [\"0\", alpha]\n    L.extend(style_factors)\n    L.extend(industry_factors)\n    return \"Ret ~ \" + \" + \".join(L)\n\nTo solve Problem 1, we need to create a function named estimate_factor_returns that performs several key tasks:\nFilter the Data: The function should first filter the input dataframe df to include only rows where IssuerMarketCap &gt; 1e9. This step ensures that the analysis focuses on liquid stocks.\nWinsorize the Return: The function should apply the winsorization process to the ‘Ret’ column of the dataframe. This step will mitigate the impact of outliers in the data.\nRun OLS Regression: Using the formula generated by get_formula function with the provided alpha factor, the function should perform an Ordinary Least Squares (OLS) regression. The dependent variable in this regression will be the winsorized return.\nReturn Results: The function should return the slope coefficients from the regression, and it can also return additional information from the regression results if desired.\nstructural exposures - Security analysts coeff - optimisation\n\nproblem 1\nWrite a function called “estimate_factor_returns” which takes as arguments, an alpha factor name, and an input data frame “df”. The function must not modify the contents of the data frame. The function will first subset to IssuerMarketCap &gt; 1e9 to obtain a liquid universe, and then run an OLS regression using the formula returned by the get_formula function defined above. The dependent variable in the regression must be winsorized return, using the winsorisation procedure discussed above in this notebook. The estimate_factor_returns function should return an object which, at very least, contains the slope coefficients from the regression, and may contain other information.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\ndef estimate_factor_returns(alpha, df):\n    # Filtering data for IssuerMarketCap &gt; 1e9\n    filtered_df = df[df['IssuerMarketCap'] &gt; 1e9]\n\n    # Winsorizing the 'Ret' column\n    winsorized_ret = wins(filtered_df['Ret'], -0.25, 0.25) # ret is return\n\n    # Running OLS regression\n    formula = get_formula(alpha)\n    model = smf.ols(formula, data=filtered_df.assign(Ret=winsorized_ret)).fit()\n\n    # Returning the slope coefficients and optionally other information\n    return model.params\n\n# Example usage\nalpha_factor_name = 'PROFIT' \ndf = frames[\"20030103\"] \nresults = estimate_factor_returns(alpha_factor_name, df)\nprint(results)\n\nPROFIT     -0.000699\nBETA       -0.000370\nSIZE       -0.001563\nMOMENTUM   -0.001272\nVALUE      -0.000369\n              ...   \nSPTYSTOR   -0.004445\nTELECOM    -0.011490\nTRADECO    -0.000577\nTRANSPRT   -0.006902\nWIRELESS    0.003033\nLength: 63, dtype: float64\n\n\n\nwins is the previously defined function for winsorization.\nget_formula is the function provided in your notebook that generates the formula for the regression.\nThe dataframe df is expected to include all the necessary columns like ‘IssuerMarketCap’, ‘Ret’, and any columns relevant to the alpha factor and style/industry factors.\n\n\n\nproblem 2\nWrite a function which iteratively calls estimate_factor_returns on every data frame in the dictionary called “frames” above, and returns a univariate time series containing the coefficient on the alpha factor for each date. Since only the alpha factor return is saved, style and industry factor returns are discarded by this procedure.\n\ndef iteratively_call_estimate_factor_return(frames, alpha_factors):\n    # DataFrame to store alpha factors and their corresponding time series\n    all_alpha_factor_returns = pd.DataFrame(index=frames.keys())\n\n    # Iterate through each date and DataFrame in the frames dictionary\n    for date, df in frames.items():\n        for alpha in alpha_factors:\n            # Call the estimate_factor_returns function for each alpha factor\n            factor_returns = estimate_factor_returns(alpha, df)\n            # Extract the alpha factor return and store it with the corresponding date\n            all_alpha_factor_returns.loc[date, alpha] = factor_returns.get(alpha, np.nan)\n\n    return all_alpha_factor_returns\n\n# Example usage\nalpha_factors = ['STREVRSL', 'PROFIT', 'SENTMT']\nalpha_time_series_df = iteratively_call_estimate_factor_return(frames, alpha_factors)\n\n\n\nproblem 3\nFor each of the potential alpha factors STREVRSL, PROFIT, SENTMT, run the function created in Problem 2 and plot the cumulative sum of the resulting time series. Do any of these qualify as potential alpha factors? Do the cumulative returns from the plot represent returns on any tradable assets or portfolios?\n\nplt.plot(pd.Series(alpha_time_series_df[\"STREVRSL\"]).cumsum(), label=\"STREVRSL\")\nplt.plot(pd.Series(alpha_time_series_df[\"PROFIT\"]).cumsum(), label=\"PROFIT\")\nplt.plot(pd.Series(alpha_time_series_df[\"SENTMT\"]).cumsum(), label=\"SENTMT\")\n\nplt.title(\"Cumulative Returns of Potential Alpha Factors\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Returns\")\nplt.legend()\nplt.xticks(rotation=45)\nplt.xticks(np.arange(0, len(alpha_time_series_df.index), 50), alpha_time_series_df.index[np.arange(0, len(alpha_time_series_df.index), 50)])\nplt.show()\n\n\n\n\nAn alpha factor is a variable or set of variables that a model uses to predict stock returns. To qualify as a potential alpha factor, a variable should ideally show a consistent ability to generate excess returns over a benchmark.\nSTREVRSL shows a steep upward trend, which suggests that this factor might have been predictive of positive returns over the period shown. If these returns are risk-adjusted and beat a benchmark, STREVRSL could be considered a strong alpha factor.\nPROFIT and SENTMT exhibit a more modest upward trajectory. This suggests that while they may have some predictive power, their ability to generate excess returns isn’t as pronounced as STREVRSL within the time frame of the data.\nPart 4: Do the cumulative returns from the plot represent returns on any tradable assets or portfolios? The cumulative returns plotted for these alpha factors represent theoretical returns based on a model’s predictions. They are not directly the returns of tradable assets but indicate how an investment strategy based on these factors might have performed. To translate these into actual tradable strategies:\nThe factors would need to be a part of a trading model that determines how much to invest in different assets.\nThe model would adjust its positions regularly based on updated factor scores and predictions.\nTransaction costs, slippage, market impact, and other real-world trading constraints would affect the actual returns realized.\nThe model’s predictions would need to be converted into portfolio weights, and these weights would then be used to construct a portfolio of stocks or other assets.\nThe plotted cumulative returns suggest how a strategy solely based on these alpha factors might have accumulated gains over time, but translating these theoretical returns into actual tradable strategies requires a comprehensive investment model and consideration of trading costs and constraints. If the factors are robust and continue to perform well out-of-sample (i.e., in future unseen data), they might be incorporated into real trading strategies or investment portfolios."
  },
  {
    "objectID": "optimisation.html#plotting-the-daily-returns-and-market-analysis",
    "href": "optimisation.html#plotting-the-daily-returns-and-market-analysis",
    "title": "Optimisation",
    "section": "Plotting the daily returns and Market Analysis",
    "text": "Plotting the daily returns and Market Analysis\n\ndf = pd.DataFrame(result_df, index=date_index)\n\n# Plotting\nplt.figure(figsize=(15, 10))\n\n# Subplot for Cumulative Profit\nplt.subplot(3, 2, 1)\nplt.plot(df['Cumulative Profit'], marker='o', color='blue')\nplt.title('Cumulative Profit')\nplt.xlabel('Date')\nplt.ylabel('Profit')\nplt.grid(True)\n\n# Subplot for Long Market Value\nplt.subplot(3, 2, 2)\nplt.plot(df['Long Market Value'], marker='o', color='green')\nplt.title('Long Market Value')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.grid(True)\n\n# Subplot for Short Market Value\nplt.subplot(3, 2, 3)\nplt.plot(df['Short Market Value'], marker='o', color='red')\nplt.title('Short Market Value')\nplt.xlabel('Date')\nplt.ylabel('Value')\nplt.grid(True)\n\n# Subplot for Daily Risk\nplt.subplot(3, 2, 4)\nplt.plot(df['Daily Risk'], marker='o', color='purple')\nplt.title('Daily Risk')\nplt.xlabel('Date')\nplt.ylabel('Risk')\nplt.grid(True)\n\n# Subplot for Idiosyncratic Risk Percent\nplt.subplot(3, 2, 5)\nplt.plot(df['Idiosyncratic Risk Percent'], marker='o', color='orange')\nplt.title('Idiosyncratic Risk Percent')\nplt.xlabel('Date')\nplt.ylabel('Percent')\nplt.grid(True)\n\n# Subplot for LS Ratio\nplt.subplot(3, 2, 6)\nplt.plot(df['LS Ratio'], marker='o', color='brown')\nplt.title('LS Ratio')\nplt.xlabel('Date')\nplt.ylabel('Ratio')\nplt.grid(True)\n\n# Adjust layout\nplt.tight_layout()\n\nplt.show()"
  }
]